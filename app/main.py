"""
Streamlit ä¸»åº”ç”¨å…¥å£
å®ç° RAG é—®ç­”ç•Œé¢ï¼Œæ”¯æŒæµå¼è¾“å‡ºå’Œæ£€ç´¢æ¥æºå±•ç¤º
"""

import os
import sys
from pathlib import Path
import streamlit as st
from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.graph import get_llm

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Financial RAG Agent",
    page_icon="ğŸ’°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ– session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "document_count" not in st.session_state:
    st.session_state.document_count = 0


@st.cache_resource
def initialize_vector_store():
    """åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆç¼“å­˜ï¼‰"""
    try:
        from langchain_huggingface import HuggingFaceEmbeddings
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        embeddings = HuggingFaceEmbeddings(
            model_name='shibing624/text2vec-base-chinese',
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        vector_db_path = project_root / "data" / "vector_db"
        client = QdrantClient(path=str(vector_db_path))
        
        vector_store = Qdrant(
            client=client,
            collection_name="financial_documents",
            embeddings=embeddings
        )
        
        # è·å–æ–‡æ¡£æ•°é‡
        try:
            collection_info = client.get_collection("financial_documents")
            document_count = collection_info.points_count
        except:
            document_count = 19085  # é»˜è®¤å€¼
        
        return vector_store, document_count
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
        return None, 0


def get_document_count():
    """è·å–æ–‡æ¡£æ•°é‡"""
    try:
        vector_db_path = project_root / "data" / "vector_db"
        client = QdrantClient(path=str(vector_db_path))
        collection_info = client.get_collection("financial_documents")
        return collection_info.points_count
    except:
        return 19085  # é»˜è®¤å€¼


def retrieve_documents(query: str, vector_store, k: int = 5):
    """
    ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        vector_store: å‘é‡å­˜å‚¨å¯¹è±¡
        k: è¿”å›çš„æ–‡æ¡£æ•°é‡
        
    Returns:
        æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
    """
    try:
        docs = vector_store.similarity_search_with_score(query, k=k)
        return docs
    except Exception as e:
        st.error(f"æ£€ç´¢æ–‡æ¡£å¤±è´¥: {str(e)}")
        return []


def format_context(docs):
    """
    æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ºä¸Šä¸‹æ–‡
    
    Args:
        docs: æ–‡æ¡£åˆ—è¡¨ï¼ˆåŒ…å«åˆ†æ•°ï¼‰
        
    Returns:
        æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²å’Œæ¥æºä¿¡æ¯
    """
    context_parts = []
    sources = []
    
    for i, (doc, score) in enumerate(docs, 1):
        content = doc.page_content
        metadata = doc.metadata
        
        # æå–æ¥æºä¿¡æ¯
        company = metadata.get("company", "æœªçŸ¥å…¬å¸")
        year = metadata.get("year", "æœªçŸ¥å¹´ä»½")
        report_type = metadata.get("report_type", "æœªçŸ¥ç±»å‹")
        file_name = metadata.get("file_name", "æœªçŸ¥æ–‡ä»¶")
        
        source_info = {
            "index": i,
            "company": company,
            "year": year,
            "report_type": report_type,
            "file_name": file_name,
            "content": content[:500] + "..." if len(content) > 500 else content,  # æˆªå–å‰500å­—ç¬¦
            "score": f"{score:.4f}"
        }
        sources.append(source_info)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts.append(f"[æ–‡æ¡£ {i}] {content}")
    
    return "\n\n".join(context_parts), sources


def generate_response(query: str, context: str, llm):
    """
    ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”ï¼ˆæµå¼è¾“å‡ºï¼‰
    
    Args:
        query: ç”¨æˆ·é—®é¢˜
        context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        llm: LLM æ¨¡å‹
        
    Yields:
        å›ç­”çš„æ–‡æœ¬ç‰‡æ®µ
    """
    from langchain_core.prompts import ChatPromptTemplate
    
    # æ„å»ºæç¤ºè¯
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. å›ç­”è¦å‡†ç¡®ã€ä¸“ä¸š
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ•°å­—å’Œæ•°æ®
4. å›ç­”è¦ç®€æ´æ˜äº†

æ–‡æ¡£å†…å®¹ï¼š
{context}"""),
        ("human", "{question}")
    ])
    
    # æ ¼å¼åŒ–æç¤ºè¯
    messages = prompt_template.format_messages(
        context=context,
        question=query
    )
    
    # æµå¼è°ƒç”¨ LLM
    for chunk in llm.stream(messages):
        # å¤„ç†ä¸åŒç±»å‹çš„ chunk
        if hasattr(chunk, 'content'):
            content = chunk.content
            if content:
                yield content
        elif isinstance(chunk, str):
            yield chunk
        elif hasattr(chunk, 'text'):
            yield chunk.text


# ä¾§è¾¹æ ï¼šæ•°æ®åº“çŠ¶æ€
with st.sidebar:
    st.header("ğŸ“Š æ•°æ®åº“çŠ¶æ€")
    
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    if st.session_state.vector_store is None:
        with st.spinner("æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“..."):
            vector_store, doc_count = initialize_vector_store()
            st.session_state.vector_store = vector_store
            st.session_state.document_count = doc_count
    
    # æ˜¾ç¤ºæ–‡æ¡£æ•°é‡
    st.metric(
        label="æ–‡æ¡£å—æ•°é‡",
        value=f"{st.session_state.document_count:,}",
        help="å½“å‰å‘é‡æ•°æ®åº“ä¸­çš„æ–‡æ¡£å—æ€»æ•°"
    )
    
    # åˆ·æ–°æŒ‰é’®
    if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€"):
        st.session_state.vector_store = None
        st.session_state.document_count = get_document_count()
        st.rerun()
    
    st.divider()
    
    st.markdown("### ğŸ“– ä½¿ç”¨è¯´æ˜")
    st.markdown("""
    1. åœ¨ä¸‹æ–¹è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜
    2. ç³»ç»Ÿä¼šä»é‡‘èæ–‡æ¡£åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯
    3. å›ç­”ä¼šä»¥æµå¼æ–¹å¼å®æ—¶æ˜¾ç¤º
    4. å›ç­”å®Œæˆåå¯æŸ¥çœ‹æ£€ç´¢æ¥æº
    """)


# ä¸»ç•Œé¢
st.title("ğŸ’° Financial RAG Agent")
st.markdown("**é‡‘èæ–‡æ¡£æ™ºèƒ½é—®ç­”ç³»ç»Ÿ** - åŸºäº LangChain å’Œ DeepSeek")

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # å¦‚æœæ˜¯åŠ©æ‰‹å›ç­”ï¼Œæ˜¾ç¤ºæ£€ç´¢æ¥æº
        if message["role"] == "assistant" and "sources" in message:
            with st.expander("ğŸ“š æ£€ç´¢æ¥æº", expanded=False):
                for source in message["sources"]:
                    st.markdown(f"""
                    **æ–‡æ¡£ {source['index']}** (ç›¸ä¼¼åº¦: {source['score']})
                    - **å…¬å¸**: {source['company']}
                    - **å¹´ä»½**: {source['year']}
                    - **æŠ¥å‘Šç±»å‹**: {source['report_type']}
                    - **æ–‡ä»¶å**: {source['file_name']}
                    - **å†…å®¹ç‰‡æ®µ**: 
                    > {source['content']}
                    """)
                    st.divider()

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å·²åˆå§‹åŒ–
    if st.session_state.vector_store is None:
        st.error("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
        st.stop()
    
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£
    with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
        docs = retrieve_documents(prompt, st.session_state.vector_store, k=5)
        
        if not docs:
            st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·å°è¯•å…¶ä»–é—®é¢˜")
            st.stop()
        
        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡å’Œæ¥æº
        context, sources = format_context(docs)
    
    # ç”Ÿæˆå›ç­”ï¼ˆæµå¼è¾“å‡ºï¼‰
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # åˆå§‹åŒ– LLM
            llm = get_llm()
            
            # æµå¼ç”Ÿæˆå›ç­”ï¼ˆæ‰“å­—æœºæ•ˆæœï¼‰
            for chunk in generate_response(prompt, context, llm):
                if chunk:  # ç¡®ä¿ chunk ä¸ä¸ºç©º
                    full_response += chunk
                    # å®æ—¶æ›´æ–°æ˜¾ç¤ºï¼Œæ·»åŠ å…‰æ ‡æ•ˆæœ
                    message_placeholder.markdown(full_response + "â–Œ")
            
            # ç§»é™¤å…‰æ ‡ï¼Œæ˜¾ç¤ºæœ€ç»ˆå›ç­”
            message_placeholder.markdown(full_response)
            
            # æ˜¾ç¤ºæ£€ç´¢æ¥æºï¼ˆä½¿ç”¨ expanderï¼‰
            with st.expander("ğŸ“š æ£€ç´¢æ¥æº", expanded=False):
                st.markdown("**ä»¥ä¸‹æ˜¯ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š**")
                st.markdown("")
                for source in sources:
                    st.markdown(f"""
                    **æ–‡æ¡£ {source['index']}** (ç›¸ä¼¼åº¦åˆ†æ•°: {source['score']})
                    - **å…¬å¸**: {source['company']}
                    - **å¹´ä»½**: {source['year']}
                    - **æŠ¥å‘Šç±»å‹**: {source['report_type']}
                    - **æ–‡ä»¶å**: `{source['file_name']}`
                    """)
                    st.markdown(f"**å†…å®¹ç‰‡æ®µï¼š**")
                    st.markdown(f"> {source['content']}")
                    st.divider()
            
            # ä¿å­˜æ¶ˆæ¯å’Œæ¥æºåˆ° session state
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response,
                "sources": sources
            })
            
        except Exception as e:
            st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
            import traceback
            with st.expander("é”™è¯¯è¯¦æƒ…"):
                st.code(traceback.format_exc())
