"""
Streamlit åº”ç”¨å¯åŠ¨å…¥å£
æ ¹ç›®å½•ä¸‹çš„ main.pyï¼Œä½œä¸º Streamlit çš„å¯åŠ¨å…¥å£ç‚¹
è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶å¹¶è°ƒç”¨ app/ ç›®å½•ä¸‹çš„æ ¸å¿ƒé€»è¾‘
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶ï¼ˆä¼˜å…ˆä»æ ¹ç›®å½•åŠ è½½ï¼‰
project_root = Path(__file__).parent
env_path = project_root / '.env'
load_dotenv(dotenv_path=env_path, override=True)

# æ£€æŸ¥ API Key æ˜¯å¦åŠ è½½æˆåŠŸï¼ˆä¸æ‰“å°æ•æ„Ÿä¿¡æ¯ï¼‰
api_key = os.getenv("OPENAI_API_KEY")
if api_key:
    print("âœ“ æˆåŠŸåŠ è½½ API Key")
else:
    print("âš  è­¦å‘Šï¼šæœªæ‰¾åˆ°æœ‰æ•ˆçš„ API Keyï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(project_root))

# å¯¼å…¥ Streamlit å’Œå¿…è¦çš„æ¨¡å—
import streamlit as st
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient

# å¯¼å…¥ app ç›®å½•ä¸‹çš„æ ¸å¿ƒæ¨¡å—
from app.graph import get_llm

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="Financial RAG Agent",
    page_icon="ğŸ’°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ– session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "document_count" not in st.session_state:
    st.session_state.document_count = 0
if "last_query_time" not in st.session_state:
    st.session_state.last_query_time = 0
if "ensemble_retriever" not in st.session_state:
    st.session_state.ensemble_retriever = None
if "reranker" not in st.session_state:
    st.session_state.reranker = None


@st.cache_resource
def initialize_vector_store():
    """åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆç¼“å­˜ï¼‰"""
    try:
        from langchain_huggingface import HuggingFaceEmbeddings
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆä¸å…¥åº“æ—¶ä¿æŒä¸€è‡´ï¼‰
        embeddings = HuggingFaceEmbeddings(
            model_name='shibing624/text2vec-base-chinese',
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨ QdrantVectorStoreï¼‰
        vector_db_path = project_root / "data" / "vector_db"
        client = QdrantClient(path=str(vector_db_path))
        
        # ä½¿ç”¨ QdrantVectorStoreï¼ˆæ–°ç‰ˆæœ¬ APIï¼‰
        vector_store = QdrantVectorStore(
            client=client,
            collection_name="financial_documents",
            embedding=embeddings
        )
        
        # è·å–æ–‡æ¡£æ•°é‡ï¼ˆä½¿ç”¨ QdrantClient ç›´æ¥æŸ¥è¯¢ï¼‰
        try:
            collection_info = client.get_collection("financial_documents")
            document_count = collection_info.points_count
        except:
            document_count = 19085  # é»˜è®¤å€¼
        
        return vector_store, document_count
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
        # ä¸æ˜¾ç¤ºè¯¦ç»†å †æ ˆè·Ÿè¸ªï¼Œé¿å…æ³„éœ²æ•æ„Ÿä¿¡æ¯
        return None, 0


def get_document_count():
    """è·å–æ–‡æ¡£æ•°é‡"""
    try:
        vector_db_path = project_root / "data" / "vector_db"
        client = QdrantClient(path=str(vector_db_path))
        collection_info = client.get_collection("financial_documents")
        return collection_info.points_count
    except:
        return 19085  # é»˜è®¤å€¼


@st.cache_resource
def initialize_ensemble_retriever(vector_store):
    """
    åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡æ£€ç´¢ + BM25ï¼‰
    
    Args:
        vector_store: å‘é‡å­˜å‚¨å¯¹è±¡
        
    Returns:
        EnsembleRetriever å¯¹è±¡
    """
    try:
        from langchain.retrievers import EnsembleRetriever
        from langchain_community.retrievers import BM25Retriever
        
        # è·å–æ‰€æœ‰æ–‡æ¡£ç”¨äº BM25ï¼ˆåˆ†æ‰¹è·å–ä»¥æé«˜æ•ˆç‡ï¼‰
        all_docs = []
        try:
            # å°è¯•è·å–æ‰€æœ‰æ–‡æ¡£ï¼Œå¦‚æœæ–‡æ¡£å¤ªå¤šåˆ™åˆ†æ‰¹å¤„ç†
            batch_size = 1000
            for i in range(0, 20000, batch_size):  # æœ€å¤šè·å– 20000 ä¸ªæ–‡æ¡£
                batch = vector_store.similarity_search("", k=batch_size)
                if not batch:
                    break
                all_docs.extend(batch)
                if len(batch) < batch_size:
                    break
        except Exception as e:
            # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨è¾ƒå°çš„æ ·æœ¬
            all_docs = vector_store.similarity_search("", k=5000)
        
        if not all_docs:
            return None
        
        # åˆå§‹åŒ– BM25 æ£€ç´¢å™¨
        bm25_retriever = BM25Retriever.from_documents(all_docs)
        bm25_retriever.k = 20  # BM25 å¬å›æ•°é‡
        
        # å‘é‡æ£€ç´¢å™¨
        vector_retriever = vector_store.as_retriever(search_kwargs={"k": 20})
        
        # æ··åˆæ£€ç´¢å™¨ï¼ˆæƒé‡å„å  0.5ï¼‰
        ensemble_retriever = EnsembleRetriever(
            retrievers=[vector_retriever, bm25_retriever],
            weights=[0.5, 0.5]
        )
        
        return ensemble_retriever
    except Exception as e:
        st.warning(f"åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨å¤±è´¥ï¼Œå°†ä½¿ç”¨çº¯å‘é‡æ£€ç´¢: {str(e)}")
        # ä¸æ˜¾ç¤ºè¯¦ç»†å †æ ˆè·Ÿè¸ªï¼Œé¿å…æ³„éœ²æ•æ„Ÿä¿¡æ¯
        return None


@st.cache_resource
def initialize_reranker():
    """
    åˆå§‹åŒ–é‡æ’åºæ¨¡å‹
    
    Returns:
        FlashrankRerank å¯¹è±¡
    """
    try:
        from langchain_community.cross_encoders import FlashrankRerank
        
        reranker = FlashrankRerank(model="ms-marco-MiniLM-L-12-v2")
        return reranker
    except Exception as e:
        st.warning(f"åˆå§‹åŒ–é‡æ’åºæ¨¡å‹å¤±è´¥: {str(e)}")
        return None


def retrieve_documents(query: str, vector_store, ensemble_retriever, reranker, k: int = 5):
    """
    ä»å‘é‡æ•°æ®åº“æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆæ”¯æŒæ··åˆæ£€ç´¢å’Œé‡æ’åºï¼‰
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        vector_store: å‘é‡å­˜å‚¨å¯¹è±¡
        ensemble_retriever: æ··åˆæ£€ç´¢å™¨
        reranker: é‡æ’åºæ¨¡å‹
        k: æœ€ç»ˆè¿”å›çš„æ–‡æ¡£æ•°é‡
        
    Returns:
        æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå¸¦åˆ†æ•°ï¼‰
    """
    try:
        if vector_store is None:
            return []
        
        # ç¬¬ä¸€æ­¥ï¼šæ··åˆæ£€ç´¢å¬å› 20 ä¸ªå€™é€‰æ–‡æ¡£
        if ensemble_retriever:
            # ä½¿ç”¨æ··åˆæ£€ç´¢
            candidate_docs = ensemble_retriever.get_relevant_documents(query)
        else:
            # é™çº§åˆ°çº¯å‘é‡æ£€ç´¢
            candidate_docs = vector_store.similarity_search(query, k=20)
        
        if not candidate_docs:
            return []
        
        # ç¬¬äºŒæ­¥ï¼šé‡æ’åºï¼Œç²¾é€‰å‡º top k ä¸ªæ–‡æ¡£
        if reranker and len(candidate_docs) > k:
            try:
                # FlashrankRerank å¯èƒ½ä½¿ç”¨ä¸åŒçš„ APIï¼Œå°è¯•å¤šç§æ–¹æ³•
                if hasattr(reranker, 'compress_documents'):
                    reranked_docs = reranker.compress_documents(
                        documents=candidate_docs,
                        query=query
                    )
                elif hasattr(reranker, 'rerank'):
                    # å°è¯• rerank æ–¹æ³•
                    reranked_docs = reranker.rerank(query, candidate_docs)
                elif hasattr(reranker, 'score'):
                    # ä½¿ç”¨ score æ–¹æ³•è¿›è¡Œé‡æ’åº
                    scored_docs = []
                    for doc in candidate_docs:
                        score = reranker.score(query, doc.page_content)
                        scored_docs.append((doc, score))
                    # æŒ‰åˆ†æ•°æ’åº
                    scored_docs.sort(key=lambda x: x[1], reverse=True)
                    reranked_docs = [doc for doc, _ in scored_docs]
                else:
                    # å¦‚æœéƒ½ä¸æ”¯æŒï¼Œä½¿ç”¨åŸå§‹æ–‡æ¡£
                    reranked_docs = candidate_docs
                
                final_docs = reranked_docs[:k] if isinstance(reranked_docs, list) else list(reranked_docs)[:k]
            except Exception as e:
                # å¦‚æœé‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æ¡£
                final_docs = candidate_docs[:k]
        else:
            final_docs = candidate_docs[:k]
        
        # è½¬æ¢ä¸ºå¸¦åˆ†æ•°çš„æ ¼å¼ï¼ˆé‡æ’åºåçš„æ–‡æ¡£æ²¡æœ‰åˆ†æ•°ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºæ’åºä¾æ®ï¼‰
        docs_with_score = []
        for idx, doc in enumerate(final_docs):
            # ä½¿ç”¨ (1.0 - idx/len(final_docs)) ä½œä¸ºç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆæ’åºè¶Šé å‰åˆ†æ•°è¶Šé«˜ï¼‰
            score = 1.0 - (idx / len(final_docs))
            docs_with_score.append((doc, score))
        
        return docs_with_score
    except Exception as e:
        st.error(f"æ£€ç´¢æ–‡æ¡£å¤±è´¥: {str(e)}")
        # ä¸æ˜¾ç¤ºè¯¦ç»†å †æ ˆè·Ÿè¸ªï¼Œé¿å…æ³„éœ²æ•æ„Ÿä¿¡æ¯
        return []


def format_context(docs):
    """
    æ ¼å¼åŒ–æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ºä¸Šä¸‹æ–‡
    
    Args:
        docs: æ–‡æ¡£åˆ—è¡¨ï¼ˆåŒ…å«åˆ†æ•°ï¼‰
        
    Returns:
        æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²å’Œæ¥æºä¿¡æ¯
    """
    context_parts = []
    sources = []
    
    for i, (doc, score) in enumerate(docs, 1):
        content = doc.page_content
        metadata = doc.metadata
        
        # æå–æ¥æºä¿¡æ¯
        company = metadata.get("company", "æœªçŸ¥å…¬å¸")
        year = metadata.get("year", "æœªçŸ¥å¹´ä»½")
        report_type = metadata.get("report_type", "æœªçŸ¥ç±»å‹")
        file_name = metadata.get("file_name", "æœªçŸ¥æ–‡ä»¶")
        
        source_info = {
            "index": i,
            "company": company,
            "year": year,
            "report_type": report_type,
            "file_name": file_name,
            "content": content[:500] + "..." if len(content) > 500 else content,  # æˆªå–å‰500å­—ç¬¦
            "score": f"{score:.4f}"
        }
        sources.append(source_info)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts.append(f"[æ–‡æ¡£ {i}] {content}")
    
    return "\n\n".join(context_parts), sources


def generate_response(query: str, context: str, llm):
    """
    ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”ï¼ˆæµå¼è¾“å‡ºï¼‰
    
    Args:
        query: ç”¨æˆ·é—®é¢˜
        context: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        llm: LLM æ¨¡å‹
        
    Yields:
        å›ç­”çš„æ–‡æœ¬ç‰‡æ®µ
    """
    from langchain_core.prompts import ChatPromptTemplate
    
    # æ„å»ºæç¤ºè¯
    prompt_template = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. å›ç­”è¦å‡†ç¡®ã€ä¸“ä¸š
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. å¯ä»¥å¼•ç”¨å…·ä½“çš„æ•°å­—å’Œæ•°æ®
4. å›ç­”è¦ç®€æ´æ˜äº†

æ–‡æ¡£å†…å®¹ï¼š
{context}"""),
        ("human", "{question}")
    ])
    
    # æ ¼å¼åŒ–æç¤ºè¯
    messages = prompt_template.format_messages(
        context=context,
        question=query
    )
    
    # æµå¼è°ƒç”¨ LLM
    for chunk in llm.stream(messages):
        # å¤„ç†ä¸åŒç±»å‹çš„ chunk
        if hasattr(chunk, 'content'):
            content = chunk.content
            if content:
                yield content
        elif isinstance(chunk, str):
            yield chunk
        elif hasattr(chunk, 'text'):
            yield chunk.text


# ä¾§è¾¹æ ï¼šä½¿ç”¨è¯´æ˜
with st.sidebar:
    st.header("ğŸ“– ä½¿ç”¨è¯´æ˜")
    
    st.markdown("### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½")
    st.markdown("""
    æœ¬åŠ©æ‰‹æ˜¯ä¸€ä¸ªä¸“ä¸šçš„**é‡‘èæ–‡æ¡£æ™ºèƒ½é—®ç­”ç³»ç»Ÿ**ï¼ŒåŸºäº RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æŠ€æœ¯æ„å»ºã€‚
    
    **ä¸»è¦èƒ½åŠ›ï¼š**
    - ğŸ“Š åˆ†æä¸Šå¸‚å…¬å¸è´¢åŠ¡æŠ¥å‘Š
    - ğŸ’° å›ç­”å…³äºè¥æ”¶ã€åˆ©æ¶¦ã€èµ„äº§ç­‰è´¢åŠ¡æŒ‡æ ‡çš„é—®é¢˜
    - ğŸ“ˆ å¯¹æ¯”ä¸åŒå…¬å¸çš„è´¢åŠ¡è¡¨ç°
    - ğŸ” æŸ¥æ‰¾ç‰¹å®šå¹´ä»½æˆ–æŠ¥å‘Šç±»å‹çš„ä¿¡æ¯
    """)
    
    st.divider()
    
    st.markdown("### ğŸ’¡ æé—®ç¤ºä¾‹")
    st.markdown("""
    **è´¢åŠ¡æŒ‡æ ‡æŸ¥è¯¢ï¼š**
    - "æ¯”äºšè¿ª2024å¹´çš„è¥ä¸šæ”¶å…¥æ˜¯å¤šå°‘ï¼Ÿ"
    - "è´µå·èŒ…å°2023å¹´çš„å‡€åˆ©æ¶¦å¢é•¿ç‡æ˜¯å¤šå°‘ï¼Ÿ"
    - "æ‹›å•†é“¶è¡Œ2024å¹´çš„æ€»èµ„äº§æ˜¯å¤šå°‘ï¼Ÿ"
    
    **å¯¹æ¯”åˆ†æï¼š**
    - "å¯¹æ¯”ä¸€ä¸‹ä¸­å›½å¹³å®‰å’Œæ‹›å•†é“¶è¡Œ2024å¹´çš„å‡€åˆ©æ¶¦"
    - "æ ¼åŠ›ç”µå™¨å’Œç«‹è®¯ç²¾å¯†2023å¹´çš„è¥æ”¶å¯¹æ¯”"
    
    **è¶‹åŠ¿åˆ†æï¼š**
    - "æµ·åº·å¨è§†è¿‘ä¸¤å¹´çš„è¥æ”¶è¶‹åŠ¿å¦‚ä½•ï¼Ÿ"
    - "ç«‹è®¯ç²¾å¯†2023åˆ°2024å¹´çš„ä¸šç»©å˜åŒ–"
    
    **å…¶ä»–é—®é¢˜ï¼š**
    - "æ¯”äºšè¿ªçš„ä¸»è¦ä¸šåŠ¡æ˜¯ä»€ä¹ˆï¼Ÿ"
    - "è´µå·èŒ…å°çš„æ ¸å¿ƒç«äº‰åŠ›æ˜¯ä»€ä¹ˆï¼Ÿ"
    """)
    
    st.divider()
    
    st.markdown("### ğŸ“š æ•°æ®æ¥æº")
    st.markdown("""
    æœ¬ç³»ç»ŸåŸºäºä»¥ä¸‹ä¸Šå¸‚å…¬å¸çš„å…¬å¼€è´¢åŠ¡æŠ¥å‘Šï¼š
    
    - **ä¸­å›½å¹³å®‰** - å¹´åº¦æŠ¥å‘Šã€åŠå¹´åº¦æŠ¥å‘Š
    - **æ‹›å•†é“¶è¡Œ** - å¹´åº¦æŠ¥å‘Šã€åŠå¹´åº¦æŠ¥å‘Š
    - **æ ¼åŠ›ç”µå™¨** - å¹´åº¦æŠ¥å‘Šã€åŠå¹´åº¦æŠ¥å‘Š
    - **æ¯”äºšè¿ª** - å¹´åº¦æŠ¥å‘Šã€åŠå¹´åº¦æŠ¥å‘Š
    - **æµ·åº·å¨è§†** - å¹´åº¦æŠ¥å‘Šã€åŠå¹´åº¦æŠ¥å‘Š
    - **ç«‹è®¯ç²¾å¯†** - å¹´åº¦æŠ¥å‘Šã€åŠå¹´åº¦æŠ¥å‘Š
    - **è´µå·èŒ…å°** - å¹´åº¦æŠ¥å‘Šã€åŠå¹´åº¦æŠ¥å‘Š
    
    æ•°æ®æ¶µç›– **2023-2025å¹´** çš„è´¢åŠ¡æŠ¥å‘Šï¼Œæ‰€æœ‰ä¿¡æ¯å‡æ¥è‡ªä¸Šå¸‚å…¬å¸å®˜æ–¹æŠ«éœ²çš„PDFæ–‡æ¡£ã€‚
    """)
    
    st.divider()
    
    st.markdown("### âš™ï¸ ä½¿ç”¨æç¤º")
    st.markdown("""
    1. **è¾“å…¥é—®é¢˜**ï¼šåœ¨ä¸‹æ–¹è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜
    2. **æ™ºèƒ½æ£€ç´¢**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨ä»æ–‡æ¡£åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯
    3. **æµå¼å›ç­”**ï¼šå›ç­”ä¼šä»¥æ‰“å­—æœºæ•ˆæœå®æ—¶æ˜¾ç¤º
    4. **æŸ¥çœ‹æ¥æº**ï¼šå›ç­”å®Œæˆåå¯å±•å¼€"æ£€ç´¢æ¥æº"æŸ¥çœ‹å…·ä½“æ–‡æ¡£ç‰‡æ®µ
    5. **å¤šè½®å¯¹è¯**ï¼šæ”¯æŒè¿ç»­æé—®ï¼Œç³»ç»Ÿä¼šè®°ä½å¯¹è¯å†å²
    """)


# ä¸»ç•Œé¢
st.title("ğŸ’° Financial RAG Agent")
st.markdown("**é‡‘èæ–‡æ¡£æ™ºèƒ½é—®ç­”ç³»ç»Ÿ** - åŸºäº LangChain å’Œ DeepSeek")

# ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
if st.session_state.vector_store is None:
    with st.spinner("æ­£åœ¨è¿æ¥é‡‘èæ•°æ®åº“..."):
        vector_store, doc_count = initialize_vector_store()
        st.session_state.vector_store = vector_store
        st.session_state.document_count = doc_count
    st.rerun()

# åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨å’Œé‡æ’åºæ¨¡å‹
if st.session_state.vector_store and st.session_state.ensemble_retriever is None:
    with st.spinner("æ­£åœ¨åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨..."):
        try:
            st.session_state.ensemble_retriever = initialize_ensemble_retriever(st.session_state.vector_store)
        except Exception as e:
            st.session_state.ensemble_retriever = None

if st.session_state.reranker is None:
    try:
        st.session_state.reranker = initialize_reranker()
    except Exception as e:
        st.session_state.reranker = None

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
        # å¦‚æœæ˜¯åŠ©æ‰‹å›ç­”ï¼Œæ˜¾ç¤ºæ£€ç´¢æ¥æº
        if message["role"] == "assistant" and "sources" in message:
            with st.expander("ğŸ“š æ£€ç´¢æ¥æº", expanded=False):
                for source in message["sources"]:
                    st.markdown(f"""
                    **æ–‡æ¡£ {source['index']}** (ç›¸ä¼¼åº¦: {source['score']})
                    - **å…¬å¸**: {source['company']}
                    - **å¹´ä»½**: {source['year']}
                    - **æŠ¥å‘Šç±»å‹**: {source['report_type']}
                    - **æ–‡ä»¶å**: {source['file_name']}
                    - **å†…å®¹ç‰‡æ®µ**: 
                    > {source['content']}
                    """)
                    st.divider()

# ç”¨æˆ·è¾“å…¥ï¼ˆç¡®ä¿åœ¨ä¸»å¾ªç¯ä¸­ï¼Œé¡µé¢åŠ è½½æ—¶å°±æ¸²æŸ“ï¼‰
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..."):
    # 1. è®¿é—®é¢‘ç‡é™åˆ¶æ£€æŸ¥
    import time
    current_time = time.time()
    time_since_last_query = current_time - st.session_state.last_query_time
    
    if time_since_last_query < 3:
        st.warning("æé—®å¤ªå¿«å•¦ï¼Œè¯·ç¨ç­‰")
        st.stop()
    
    # æ›´æ–°æœ€åæŸ¥è¯¢æ—¶é—´
    st.session_state.last_query_time = current_time
    
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å·²åˆå§‹åŒ–
    if st.session_state.vector_store is None:
        st.error("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
        st.stop()
    
    # æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆä½¿ç”¨æ··åˆæ£€ç´¢å’Œé‡æ’åºï¼‰
    with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
        docs = retrieve_documents(
            prompt, 
            st.session_state.vector_store,
            st.session_state.ensemble_retriever,
            st.session_state.reranker,
            k=5
        )
        
        if not docs:
            st.warning("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·å°è¯•å…¶ä»–é—®é¢˜")
            st.stop()
        
        # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡å’Œæ¥æº
        context, sources = format_context(docs)
    
    # ç”Ÿæˆå›ç­”ï¼ˆæµå¼è¾“å‡ºï¼‰
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        try:
            # åˆå§‹åŒ– LLMï¼ˆæ¯æ¬¡è°ƒç”¨æ—¶é‡æ–°åŠ è½½ï¼Œç¡®ä¿ä½¿ç”¨æœ€æ–°çš„ API Keyï¼‰
            llm = get_llm()
            
            # æµå¼ç”Ÿæˆå›ç­”ï¼ˆæ‰“å­—æœºæ•ˆæœï¼‰
            for chunk in generate_response(prompt, context, llm):
                if chunk:  # ç¡®ä¿ chunk ä¸ä¸ºç©º
                    full_response += chunk
                    # å®æ—¶æ›´æ–°æ˜¾ç¤ºï¼Œæ·»åŠ å…‰æ ‡æ•ˆæœ
                    message_placeholder.markdown(full_response + "â–Œ")
            
            # ç§»é™¤å…‰æ ‡ï¼Œæ˜¾ç¤ºæœ€ç»ˆå›ç­”
            message_placeholder.markdown(full_response)
            
            # æ˜¾ç¤ºæ£€ç´¢æ¥æºï¼ˆä½¿ç”¨ expanderï¼‰
            with st.expander("ğŸ“š æ£€ç´¢æ¥æº", expanded=False):
                st.markdown("**ä»¥ä¸‹æ˜¯ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š**")
                st.markdown("")
                for source in sources:
                    st.markdown(f"""
                    **æ–‡æ¡£ {source['index']}** (ç›¸ä¼¼åº¦åˆ†æ•°: {source['score']})
                    - **å…¬å¸**: {source['company']}
                    - **å¹´ä»½**: {source['year']}
                    - **æŠ¥å‘Šç±»å‹**: {source['report_type']}
                    - **æ–‡ä»¶å**: `{source['file_name']}`
                    """)
                    st.markdown(f"**å†…å®¹ç‰‡æ®µï¼š**")
                    st.markdown(f"> {source['content']}")
                    st.divider()
            
            # ä¿å­˜æ¶ˆæ¯å’Œæ¥æºåˆ° session state
            st.session_state.messages.append({
                "role": "assistant",
                "content": full_response,
                "sources": sources
            })
            
        except Exception as e:
            st.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
            # ä¸æ˜¾ç¤ºè¯¦ç»†å †æ ˆè·Ÿè¸ªï¼Œé¿å…æ³„éœ²æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚ API Keyã€æ–‡ä»¶è·¯å¾„ç­‰ï¼‰
