"""
æ•°æ®å¯¼å…¥è„šæœ¬
è‡ªåŠ¨æ‰«æ data/raw/ æ–‡ä»¶å¤¹ï¼Œæå–å…ƒæ•°æ®ï¼Œå¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
"""

import os
import sys
import re
import logging
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
from tqdm import tqdm
from dotenv import load_dotenv

# å¯¼å…¥ torch ç”¨äº GPU æ£€æµ‹
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None

# è‡ªåŠ¨å¯»æ‰¾å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ä¸‹çš„ .env
env_path = Path(__file__).parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

# å¢åŠ æ£€æŸ¥é€»è¾‘ï¼ˆä¸æ‰“å°ä»»ä½•æ•æ„Ÿä¿¡æ¯ï¼‰
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°æœ‰æ•ˆçš„ API Keyï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶")
else:
    print("âœ“ æˆåŠŸåŠ è½½ API Key")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from langchain_community.vectorstores import Qdrant
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, Filter, FieldCondition, MatchValue

# å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ langchain
try:
    from langchain_core.documents import Document
except ImportError:
    try:
        from langchain.schema import Document
    except ImportError:
        from langchain.docstore.document import Document

from app.pdf_processor import PDFProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ingest.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ£€æµ‹è®¡ç®—è®¾å¤‡ï¼ˆGPU/CPUï¼‰
def get_device():
    """
    è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„è®¡ç®—è®¾å¤‡
    
    Returns:
        str: 'cuda' æˆ– 'cpu'
    """
    if TORCH_AVAILABLE and torch.cuda.is_available():
        device = "cuda"
        device_name = torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else "CUDA"
        logger.info(f"ğŸš€ å½“å‰è®¡ç®—è®¾å¤‡: CUDA ({device_name})")
        print(f"ğŸš€ å½“å‰è®¡ç®—è®¾å¤‡: CUDA ({device_name})")
    else:
        device = "cpu"
        if not TORCH_AVAILABLE:
            logger.warning("PyTorch æœªå®‰è£…ï¼Œä½¿ç”¨ CPU æ¨¡å¼")
            print("âš ï¸  PyTorch æœªå®‰è£…ï¼Œä½¿ç”¨ CPU æ¨¡å¼")
        elif not torch.cuda.is_available():
            logger.info("ğŸš€ å½“å‰è®¡ç®—è®¾å¤‡: CPU (CUDA ä¸å¯ç”¨)")
            print("ğŸš€ å½“å‰è®¡ç®—è®¾å¤‡: CPU (CUDA ä¸å¯ç”¨)")
        else:
            logger.info("ğŸš€ å½“å‰è®¡ç®—è®¾å¤‡: CPU")
            print("ğŸš€ å½“å‰è®¡ç®—è®¾å¤‡: CPU")
    return device

# åœ¨ç¨‹åºå¯åŠ¨æ—¶æ£€æµ‹è®¾å¤‡
DEVICE = get_device()


class DataIngester:
    """æ•°æ®å¯¼å…¥å™¨"""
    
    def __init__(
        self,
        raw_data_dir: str = "data/raw",
        vector_db_path: str = "data/vector_db",
        collection_name: str = "financial_documents",
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ):
        """
        åˆå§‹åŒ–æ•°æ®å¯¼å…¥å™¨
        
        Args:
            raw_data_dir: åŸå§‹æ•°æ®ç›®å½•
            vector_db_path: å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„
            collection_name: Qdrant é›†åˆåç§°
            chunk_size: æ–‡æ¡£å—å¤§å°
            chunk_overlap: å—é‡å å¤§å°
        """
        self.raw_data_dir = Path(raw_data_dir)
        self.vector_db_path = Path(vector_db_path)
        self.collection_name = collection_name
        self.pdf_processor = PDFProcessor(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = self._initialize_embeddings()
        
        # åˆå§‹åŒ– Qdrant å®¢æˆ·ç«¯ï¼ˆç”¨äºæŸ¥è¯¢å…ƒæ•°æ®ï¼‰
        self.client = None
        
        # ç¡®ä¿å‘é‡æ•°æ®åº“ç›®å½•å­˜åœ¨
        self.vector_db_path.mkdir(parents=True, exist_ok=True)
    
    def _initialize_embeddings(self):
        """
        åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        ç›´æ¥ä½¿ç”¨æœ¬åœ° HuggingFaceEmbeddings æ¨¡å‹ï¼ˆä¸æ¶ˆè€— API é¢åº¦ï¼‰
        æ”¯æŒ GPU åŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
        
        Returns:
            åµŒå…¥æ¨¡å‹å¯¹è±¡
        """
        logger.info("åˆå§‹åŒ–æœ¬åœ° HuggingFaceEmbeddings æ¨¡å‹...")
        
        try:
            # ä¼˜å…ˆä½¿ç”¨ langchain_huggingfaceï¼ˆæ¨èï¼‰
            try:
                from langchain_huggingface import HuggingFaceEmbeddings
                logger.info("ä½¿ç”¨ langchain_huggingface åŒ…")
            except ImportError:
                # å…¼å®¹æ—§ç‰ˆæœ¬çš„ langchain
                try:
                    from langchain_community.embeddings import HuggingFaceEmbeddings
                    logger.info("ä½¿ç”¨ langchain_community.embeddings")
                except ImportError:
                    from langchain.embeddings import HuggingFaceEmbeddings
                    logger.info("ä½¿ç”¨ langchain.embeddings")
            
            # ä½¿ç”¨å…¨å±€æ£€æµ‹åˆ°çš„è®¾å¤‡
            device = DEVICE
            logger.info(f"åŠ è½½æ¨¡å‹: shibing624/text2vec-base-chinese (è®¾å¤‡: {device})")
            embeddings = HuggingFaceEmbeddings(
                model_name='shibing624/text2vec-base-chinese',
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True}
            )
            
            # æµ‹è¯•åµŒå…¥æ¨¡å‹æ˜¯å¦å¯ç”¨
            logger.info("æµ‹è¯•åµŒå…¥æ¨¡å‹...")
            test_text = "æµ‹è¯•"
            _ = embeddings.embed_query(test_text)
            logger.info(f"âœ“ æˆåŠŸåˆå§‹åŒ– HuggingFaceEmbeddingsï¼ˆæœ¬åœ°æ¨¡å‹ï¼Œè®¾å¤‡: {device}ï¼Œä¸æ¶ˆè€— API é¢åº¦ï¼‰")
            
            return embeddings
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– HuggingFaceEmbeddings å¤±è´¥: {str(e)}")
            # å¦‚æœ GPU åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•é™çº§åˆ° CPU
            if DEVICE == "cuda":
                logger.warning("GPU åˆå§‹åŒ–å¤±è´¥ï¼Œå°è¯•é™çº§åˆ° CPU...")
                try:
                    embeddings = HuggingFaceEmbeddings(
                        model_name='shibing624/text2vec-base-chinese',
                        model_kwargs={'device': 'cpu'},
                        encode_kwargs={'normalize_embeddings': True}
                    )
                    _ = embeddings.embed_query("æµ‹è¯•")
                    logger.info("âœ“ æˆåŠŸé™çº§åˆ° CPU æ¨¡å¼")
                    return embeddings
                except Exception as e2:
                    logger.error(f"CPU é™çº§ä¹Ÿå¤±è´¥: {str(e2)}")
            raise Exception(f"æ— æ³•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {str(e)}")
    
    def extract_metadata_from_path(self, file_path: Path) -> Dict[str, str]:
        """
        ä»æ–‡ä»¶è·¯å¾„æå–å…ƒæ•°æ®
        
        Args:
            file_path: PDF æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŒ…å« company, year, report_type çš„å…ƒæ•°æ®å­—å…¸
        """
        metadata = {
            "company": "",
            "year": "",
            "report_type": "",
            "file_name": file_path.name,
            "file_path": str(file_path)
        }
        
        # æå–å…¬å¸åï¼ˆçˆ¶ç›®å½•åï¼‰
        if file_path.parent.name:
            metadata["company"] = file_path.parent.name
        
        # ä»æ–‡ä»¶åæå–å¹´ä»½å’ŒæŠ¥å‘Šç±»å‹
        file_name = file_path.stem  # ä¸å«æ‰©å±•å
        
        # ä¸­æ–‡æ•°å­—åˆ°é˜¿æ‹‰ä¼¯æ•°å­—çš„æ˜ å°„
        chinese_digit_map = {
            'é›¶': '0', 'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4',
            'äº”': '5', 'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9',
            'å': '10'
        }
        
        year = None
        
        # ä¼˜å…ˆåŒ¹é…4ä½é˜¿æ‹‰ä¼¯æ•°å­—å¹´ä»½
        four_digit_match = re.search(r'(\d{4})', file_name)
        if four_digit_match:
            year = four_digit_match.group(1)
        else:
            # åŒ¹é…ä¸­æ–‡å¹´ä»½ï¼ˆå¦‚ï¼šäºŒé›¶äºŒä¸‰ã€äºŒé›¶äºŒå››ï¼‰
            chinese_year_patterns = [
                r'äºŒé›¶([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])',  # äºŒé›¶äºŒX
                r'äºŒé›¶([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])',  # äºŒé›¶XX
            ]
            
            for pattern in chinese_year_patterns:
                match = re.search(pattern, file_name)
                if match:
                    if len(match.groups()) == 1:
                        # äºŒé›¶äºŒX æ ¼å¼
                        digit = chinese_digit_map.get(match.group(1), '0')
                        year = f"202{digit}"
                    else:
                        # äºŒé›¶XX æ ¼å¼
                        tens = chinese_digit_map.get(match.group(1), '0')
                        ones = chinese_digit_map.get(match.group(2), '0')
                        if tens == '10':
                            year = f"20{ones}"
                        else:
                            year = f"20{tens}{ones}"
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾2ä½é˜¿æ‹‰ä¼¯æ•°å­—å¹´ä»½å¹¶è½¬æ¢
            if not year:
                two_digit_match = re.search(r'(\d{2})', file_name)
                if two_digit_match:
                    two_digit = int(two_digit_match.group(1))
                    # å‡è®¾æ˜¯2000-2099å¹´
                    if two_digit < 50:
                        year = f"20{two_digit:02d}"
                    else:
                        year = f"19{two_digit:02d}"
        
        metadata["year"] = year if year else "æœªçŸ¥"
        
        # æå–æŠ¥å‘Šç±»å‹
        report_type = "æœªçŸ¥"
        if "å¹´åº¦" in file_name or "å¹´æŠ¥" in file_name:
            report_type = "å¹´åº¦æŠ¥å‘Š"
        elif "åŠå¹´åº¦" in file_name or "åŠå¹´æŠ¥" in file_name:
            report_type = "åŠå¹´åº¦æŠ¥å‘Š"
        elif "å­£åº¦" in file_name or "å­£æŠ¥" in file_name:
            report_type = "å­£åº¦æŠ¥å‘Š"
        
        metadata["report_type"] = report_type
        
        return metadata
    
    def scan_pdf_files(self) -> List[Path]:
        """
        æ‰«æ data/raw/ ç›®å½•ä¸‹çš„æ‰€æœ‰ PDF æ–‡ä»¶
        
        Returns:
            PDF æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        pdf_files = []
        
        if not self.raw_data_dir.exists():
            logger.error(f"åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.raw_data_dir}")
            return pdf_files
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ PDF æ–‡ä»¶
        for pdf_file in self.raw_data_dir.rglob("*.pdf"):
            pdf_files.append(pdf_file)
        
        logger.info(f"æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶")
        return sorted(pdf_files)
    
    def check_file_exists(self, file_hash: str) -> bool:
        """
        æ£€æŸ¥ Qdrant ä¸­æ˜¯å¦å·²å­˜åœ¨å…·æœ‰ç›¸åŒ file_hash çš„è®°å½•
        
        Args:
            file_hash: æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼
            
        Returns:
            å¦‚æœå­˜åœ¨è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """
        if self.client is None:
            return False
        
        try:
            # ä½¿ç”¨ Qdrant çš„ scroll æ–¹æ³•é…åˆè¿‡æ»¤å™¨æŸ¥è¯¢
            scroll_filter = Filter(
                must=[
                    FieldCondition(
                        key="file_hash",
                        match=MatchValue(value=file_hash)
                    )
                ]
            )
            
            # åªæŸ¥è¯¢ä¸€æ¡è®°å½•å³å¯åˆ¤æ–­æ˜¯å¦å­˜åœ¨
            results, _ = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=scroll_filter,
                limit=1
            )
            
            return len(results) > 0
        except Exception as e:
            # å¦‚æœæŸ¥è¯¢å¤±è´¥ï¼ˆå¯èƒ½æ˜¯é›†åˆä¸å­˜åœ¨æˆ–å­—æ®µä¸å­˜åœ¨ï¼‰ï¼Œè¿”å› False
            logger.debug(f"æŸ¥è¯¢æ–‡ä»¶å“ˆå¸Œæ—¶å‡ºé”™ï¼ˆå¯èƒ½é›†åˆä¸ºç©ºï¼‰: {str(e)}")
            return False
    
    def generate_deterministic_id(self, file_name: str, chunk_index: int):
        """
        ç”Ÿæˆç¡®å®šæ€§ UUIDï¼ˆåŸºäºæ–‡ä»¶åå’Œåˆ†å—åºå·ï¼‰
        ç¡®ä¿åŒä¸€ä¸ªå—å¤šæ¬¡å†™å…¥æ—¶ä½¿ç”¨ç›¸åŒçš„ IDï¼Œå®ç° upsert
        
        Args:
            file_name: æ–‡ä»¶å
            chunk_index: åˆ†å—åºå·
            
        Returns:
            UUID å¯¹è±¡ï¼ˆQdrant æ”¯æŒ UUID å¯¹è±¡ä½œä¸º IDï¼‰
        """
        # ä½¿ç”¨æ–‡ä»¶åå’Œåˆ†å—åºå·ç”Ÿæˆç¡®å®šæ€§ UUID
        namespace = uuid.NAMESPACE_DNS
        unique_string = f"{file_name}_{chunk_index}"
        deterministic_uuid = uuid.uuid5(namespace, unique_string)
        return deterministic_uuid
    
    def process_file(self, pdf_path: Path, vector_store: Optional[Qdrant] = None) -> List[Document]:
        """
        å¤„ç†å•ä¸ª PDF æ–‡ä»¶ï¼ˆæ”¯æŒå¢é‡å…¥åº“å’Œå»é‡ï¼‰
        
        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            vector_store: å‘é‡å­˜å‚¨å¯¹è±¡ï¼ˆç”¨äºæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼‰
            
        Returns:
            Document å¯¹è±¡åˆ—è¡¨ï¼ˆå¦‚æœæ–‡ä»¶å·²å­˜åœ¨åˆ™è¿”å›ç©ºåˆ—è¡¨ï¼‰
        """
        try:
            logger.info(f"æ­£åœ¨æ£€æŸ¥: {pdf_path.name}")
            
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ï¼ˆæŒ‡çº¹ï¼‰
            file_hash = self.pdf_processor.calculate_file_hash(str(pdf_path))
            logger.debug(f"æ–‡ä»¶å“ˆå¸Œ: {file_hash[:8]}...")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if self.check_file_exists(file_hash):
                logger.info(f"[SKIP] æ–‡ä»¶ {pdf_path.name} å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
                return []
            
            logger.info(f"[NEW] å¤„ç†æ–°æ–‡ä»¶: {pdf_path.name}")
            
            # æå–å…ƒæ•°æ®
            metadata = self.extract_metadata_from_path(pdf_path)
            
            # å¤„ç† PDF
            chunks = self.pdf_processor.process_pdf(str(pdf_path))
            
            # è·å–å½“å‰æ—¶é—´æˆ³
            processed_at = datetime.now().isoformat()
            
            # è½¬æ¢ä¸º Document å¯¹è±¡ï¼Œå¢åŠ å…ƒæ•°æ®å­—æ®µ
            documents = []
            for idx, chunk in enumerate(chunks):
                # ç”Ÿæˆç¡®å®šæ€§ ID
                doc_id = self.generate_deterministic_id(pdf_path.name, idx)
                
                doc = Document(
                    page_content=chunk["text"],
                    metadata={
                        **metadata,
                        "file_hash": file_hash,
                        "processed_at": processed_at,
                        "source_file": pdf_path.name,
                        "chunk_index": idx,
                        "total_chunks": len(chunks),
                        "doc_id": str(doc_id)  # å­˜å‚¨ä¸ºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨æ—¶è½¬æ¢ä¸º UUID
                    }
                )
                documents.append(doc)
            
            logger.info(f"æˆåŠŸå¤„ç† {pdf_path.name}: ç”Ÿæˆ {len(documents)} ä¸ªæ–‡æ¡£å—")
            return documents
        
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {pdf_path} æ—¶å‡ºé”™: {str(e)}")
            return []
    
    def initialize_vector_store(self) -> Qdrant:
        """
        åˆå§‹åŒ–æˆ–åŠ è½½å‘é‡æ•°æ®åº“
        
        Returns:
            Qdrant å‘é‡å­˜å‚¨å¯¹è±¡
        """
        try:
            # ä½¿ç”¨æœ¬åœ° Qdrant å®¢æˆ·ç«¯
            client = QdrantClient(path=str(self.vector_db_path))
            
            # ä¿å­˜å®¢æˆ·ç«¯å¼•ç”¨ï¼Œç”¨äºåç»­æŸ¥è¯¢
            self.client = client
            
            # æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
            collections = client.get_collections().collections
            collection_names = [col.name for col in collections]
            
            if self.collection_name not in collection_names:
                # åˆ›å»ºæ–°é›†åˆ
                # åŠ¨æ€è·å–åµŒå…¥ç»´åº¦
                try:
                    test_embedding = self.embeddings.embed_query("test")
                    embedding_dim = len(test_embedding)
                    logger.info(f"æ£€æµ‹åˆ°åµŒå…¥ç»´åº¦: {embedding_dim}")
                except Exception as e:
                    # å¦‚æœæ— æ³•è·å–ï¼Œä½¿ç”¨ HuggingFace æ¨¡å‹çš„é»˜è®¤ç»´åº¦
                    logger.warning(f"æ— æ³•è‡ªåŠ¨æ£€æµ‹åµŒå…¥ç»´åº¦: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                    embedding_dim = 768  # HuggingFace text2vec-base-chinese ç»´åº¦
                
                client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=embedding_dim,
                        distance=Distance.COSINE
                    )
                )
                logger.info(f"åˆ›å»ºæ–°é›†åˆ: {self.collection_name} (ç»´åº¦: {embedding_dim})")
            else:
                logger.info(f"ä½¿ç”¨ç°æœ‰é›†åˆ: {self.collection_name}")
            
            # åˆ›å»ºå‘é‡å­˜å‚¨
            vector_store = Qdrant(
                client=client,
                collection_name=self.collection_name,
                embeddings=self.embeddings
            )
            
            return vector_store
        
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            raise
    
    def ingest(self, reset_collection: bool = False):
        """
        æ‰§è¡Œæ•°æ®å¯¼å…¥
        
        Args:
            reset_collection: æ˜¯å¦é‡ç½®é›†åˆï¼ˆåˆ é™¤ç°æœ‰æ•°æ®ï¼‰
        """
        logger.info("=" * 60)
        logger.info("å¼€å§‹æ•°æ®å¯¼å…¥æµç¨‹")
        logger.info("=" * 60)
        
        # æ‰«æ PDF æ–‡ä»¶
        pdf_files = self.scan_pdf_files()
        
        if len(pdf_files) == 0:
            logger.warning("æœªæ‰¾åˆ°ä»»ä½• PDF æ–‡ä»¶")
            return
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        if reset_collection:
            logger.info("é‡ç½®å‘é‡æ•°æ®åº“é›†åˆ...")
            try:
                client = QdrantClient(path=str(self.vector_db_path))
                client.delete_collection(self.collection_name)
                logger.info(f"å·²åˆ é™¤é›†åˆ: {self.collection_name}")
            except Exception as e:
                logger.warning(f"åˆ é™¤é›†åˆæ—¶å‡ºé”™ï¼ˆå¯èƒ½ä¸å­˜åœ¨ï¼‰: {str(e)}")
        
        vector_store = self.initialize_vector_store()
        
        # å¤„ç†æ‰€æœ‰æ–‡ä»¶ï¼ˆå¢é‡å…¥åº“ï¼‰
        all_documents = []
        skipped_files = 0
        new_files = 0
        
        with tqdm(total=len(pdf_files), desc="æ£€æŸ¥ PDF æ–‡ä»¶", unit="æ–‡ä»¶") as pbar:
            for pdf_path in pdf_files:
                documents = self.process_file(pdf_path, vector_store)
                if len(documents) == 0:
                    skipped_files += 1
                else:
                    new_files += 1
                    all_documents.extend(documents)
                pbar.update(1)
        
        # ç»Ÿè®¡ä¿¡æ¯
        logger.info("=" * 60)
        logger.info("æ–‡ä»¶æ£€æŸ¥å®Œæˆ")
        logger.info(f"  æ–°å¢æ–‡ä»¶: {new_files} ä¸ª")
        logger.info(f"  è·³è¿‡æ–‡ä»¶: {skipped_files} ä¸ªï¼ˆå·²å­˜åœ¨ï¼‰")
        logger.info(f"  æ€»è®¡æ–‡ä»¶: {len(pdf_files)} ä¸ª")
        logger.info("=" * 60)
        
        if len(all_documents) == 0:
            logger.warning("æ²¡æœ‰éœ€è¦å…¥åº“çš„æ–°æ–‡æ¡£å—")
            logger.info("=" * 60)
            logger.info("æ•°æ®å¯¼å…¥å®Œæˆï¼")
            logger.info(f"æ€»è®¡å¤„ç†: {len(pdf_files)} ä¸ªæ–‡ä»¶")
            logger.info(f"æ–°å¢æ–‡ä»¶: {new_files} ä¸ª")
            logger.info(f"è·³è¿‡æ–‡ä»¶: {skipped_files} ä¸ª")
            logger.info(f"æ–°å¢æ–‡æ¡£å—: 0 ä¸ª")
            logger.info(f"å‘é‡æ•°æ®åº“è·¯å¾„: {self.vector_db_path}")
            logger.info(f"é›†åˆåç§°: {self.collection_name}")
            logger.info("=" * 60)
            return
        
        # æ‰¹é‡æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨ upsert æ¨¡å¼ï¼‰
        logger.info(f"æ­£åœ¨å°† {len(all_documents)} ä¸ªæ–‡æ¡£å—æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ï¼ˆä½¿ç”¨ upsert æ¨¡å¼ï¼‰...")
        
        # ä½¿ç”¨ QdrantClient ç›´æ¥å®ç° upsertï¼ˆæ”¯æŒç¡®å®šæ€§ IDï¼‰
        from qdrant_client.models import PointStruct
        
        with tqdm(total=len(all_documents), desc="å¯¼å…¥å‘é‡æ•°æ®åº“", unit="å—") as pbar:
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜é—®é¢˜
            batch_size = 100
            for i in range(0, len(all_documents), batch_size):
                batch = all_documents[i:i + batch_size]
                try:
                    # å‡†å¤‡ upsert çš„ç‚¹
                    points = []
                    for doc in batch:
                        # ç”Ÿæˆå‘é‡åµŒå…¥
                        embedding = self.embeddings.embed_query(doc.page_content)
                        
                        # è·å–ç¡®å®šæ€§ ID
                        doc_id = doc.metadata.get("doc_id")
                        if not doc_id:
                            # å¦‚æœæ²¡æœ‰ doc_idï¼Œä½¿ç”¨æ–‡ä»¶åå’Œåˆ†å—åºå·ç”Ÿæˆ
                            file_name = doc.metadata.get("source_file", "unknown")
                            chunk_index = doc.metadata.get("chunk_index", 0)
                            doc_id = self.generate_deterministic_id(file_name, chunk_index)
                        else:
                            # å¦‚æœ doc_id æ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸º UUID å¯¹è±¡
                            if isinstance(doc_id, str):
                                try:
                                    doc_id = uuid.UUID(doc_id)
                                except ValueError:
                                    # å¦‚æœæ— æ³•è§£æä¸º UUIDï¼Œé‡æ–°ç”Ÿæˆ
                                    file_name = doc.metadata.get("source_file", "unknown")
                                    chunk_index = doc.metadata.get("chunk_index", 0)
                                    doc_id = self.generate_deterministic_id(file_name, chunk_index)
                        
                        # å°†å…ƒæ•°æ®è½¬æ¢ä¸º Qdrant æ ¼å¼ï¼ˆæ‰€æœ‰å€¼å¿…é¡»æ˜¯åŸºæœ¬ç±»å‹ï¼‰
                        # æ³¨æ„ï¼šdoc_id ä¸ä½œä¸º payloadï¼Œè€Œæ˜¯ä½œä¸º PointStruct çš„ id
                        payload = {}
                        for key, value in doc.metadata.items():
                            # è·³è¿‡ doc_idï¼Œå› ä¸ºå®ƒå·²ç»ä½œä¸º PointStruct çš„ id
                            if key == "doc_id":
                                continue
                            # Qdrant åªæ”¯æŒåŸºæœ¬ç±»å‹ï¼ˆstr, int, float, bool, list, dictï¼‰
                            if isinstance(value, (str, int, float, bool)):
                                payload[key] = value
                            elif value is None:
                                payload[key] = None
                            else:
                                # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                payload[key] = str(value)
                        
                        # åˆ›å»º PointStruct
                        point = PointStruct(
                            id=doc_id,
                            vector=embedding,
                            payload=payload
                        )
                        points.append(point)
                    
                    # ä½¿ç”¨ upsert æ–¹æ³•ï¼ˆå¦‚æœ ID å·²å­˜åœ¨åˆ™æ›´æ–°ï¼Œå¦åˆ™æ’å…¥ï¼‰
                    self.client.upsert(
                        collection_name=self.collection_name,
                        points=points
                    )
                    pbar.update(len(batch))
                except Exception as e:
                    logger.error(f"æ·»åŠ æ–‡æ¡£æ‰¹æ¬¡æ—¶å‡ºé”™: {str(e)}")
                    # å¦‚æœç›´æ¥ upsert å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ LangChain çš„ add_documents ä½œä¸ºåå¤‡
                    try:
                        logger.warning("å°è¯•ä½¿ç”¨ LangChain çš„ add_documents ä½œä¸ºåå¤‡æ–¹æ¡ˆ...")
                        vector_store.add_documents(batch)
                        pbar.update(len(batch))
                    except Exception as e2:
                        logger.error(f"åå¤‡æ–¹æ¡ˆä¹Ÿå¤±è´¥: {str(e2)}")
                        pbar.update(len(batch))
        
        logger.info("=" * 60)
        logger.info("æ•°æ®å¯¼å…¥å®Œæˆï¼")
        logger.info(f"æ€»è®¡å¤„ç†: {len(pdf_files)} ä¸ªæ–‡ä»¶")
        logger.info(f"  æ–°å¢æ–‡ä»¶: {new_files} ä¸ª")
        logger.info(f"  è·³è¿‡æ–‡ä»¶: {skipped_files} ä¸ª")
        logger.info(f"æ–°å¢æ–‡æ¡£å—: {len(all_documents)} ä¸ª")
        logger.info(f"å‘é‡æ•°æ®åº“è·¯å¾„: {self.vector_db_path}")
        logger.info(f"é›†åˆåç§°: {self.collection_name}")
        logger.info("=" * 60)
        
        # é‡Šæ”¾ GPU æ˜¾å­˜
        if TORCH_AVAILABLE and torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("âœ“ å·²é‡Šæ”¾ GPU æ˜¾å­˜")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    # ç¯å¢ƒå˜é‡å·²åœ¨æ–‡ä»¶å¼€å¤´åŠ è½½ï¼Œè¿™é‡Œè¿›è¡Œæœ€ç»ˆæ£€æŸ¥
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("=" * 60)
        print("é”™è¯¯ï¼šæœªæ£€æµ‹åˆ° .env ä¸­çš„ API Key")
        print("=" * 60)
        print(f"è¯·æ£€æŸ¥ .env æ–‡ä»¶æ˜¯å¦å­˜åœ¨: {env_path}")
        print("ç¡®ä¿ .env æ–‡ä»¶åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š")
        print("  OPENAI_API_KEY=your_api_key_here")
        print("  OPENAI_API_BASE=your_api_base_url")
        print("  MODEL_NAME=your_model_name")
        print("=" * 60)
        print("æç¤ºï¼šè¯·å‚è€ƒ .env.example æ–‡ä»¶è¿›è¡Œé…ç½®")
        print("=" * 60)
        sys.exit(1)
    
    # æ‰“å°ç¯å¢ƒå˜é‡ä¿¡æ¯ï¼ˆå¥å£®æ€§æ£€æŸ¥ï¼Œä¸æ‰“å°æ•æ„Ÿä¿¡æ¯ï¼‰
    model_name = os.getenv("MODEL_NAME")
    print("=" * 60)
    print("ç¯å¢ƒå˜é‡æ£€æŸ¥ï¼š")
    if model_name:
        print(f"  MODEL_NAME: {model_name}")
    print(f"  API Key: {'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'}")
    print("=" * 60)
    
    parser = argparse.ArgumentParser(description="å¯¼å…¥ PDF æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
    parser.add_argument(
        "--raw-dir",
        type=str,
        default="data/raw",
        help="åŸå§‹æ•°æ®ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤: data/rawï¼‰"
    )
    parser.add_argument(
        "--vector-db-path",
        type=str,
        default="data/vector_db",
        help="å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„ï¼ˆé»˜è®¤: data/vector_dbï¼‰"
    )
    parser.add_argument(
        "--collection-name",
        type=str,
        default="financial_documents",
        help="Qdrant é›†åˆåç§°ï¼ˆé»˜è®¤: financial_documentsï¼‰"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=1000,
        help="æ–‡æ¡£å—å¤§å°ï¼ˆé»˜è®¤: 1000ï¼‰"
    )
    parser.add_argument(
        "--chunk-overlap",
        type=int,
        default=200,
        help="å—é‡å å¤§å°ï¼ˆé»˜è®¤: 200ï¼‰"
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="é‡ç½®é›†åˆï¼ˆåˆ é™¤ç°æœ‰æ•°æ®ï¼‰"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯¼å…¥å™¨å¹¶æ‰§è¡Œå¯¼å…¥
    ingester = DataIngester(
        raw_data_dir=args.raw_dir,
        vector_db_path=args.vector_db_path,
        collection_name=args.collection_name,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap
    )
    
    ingester.ingest(reset_collection=args.reset)


if __name__ == "__main__":
    main()
